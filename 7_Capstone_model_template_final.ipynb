{"cells":[{"cell_type":"code","execution_count":152,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1745257734153,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"ErSwyrY0PHQY"},"outputs":[],"source":["# Final Revision of Capstone Modeling Process"]},{"cell_type":"code","execution_count":153,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1745257734171,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"Nwp62HTPdY-R"},"outputs":[],"source":["# Modeling Template File Notes:\n","# File Sequence #7\n","\n","# the purpose of this notebook is to\n","\n","# input:\n","# 1. For a given Fold:\n","## A. Labeled Seen Dataset\n","## B. Labeled Unseen Dataset\n","# 2. User Model Selection\n","# 3. User Feature Set Selection (if Fold 1)\n","# 4. Desired predicted field (y-val)\n","\n","# output:\n","# 1. Testing Dataset with Predicted Labels and Actual Labels\n","# (Per ticker, Per day)"]},{"cell_type":"code","execution_count":154,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3727,"status":"ok","timestamp":1745257737898,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"9OkARY7Qd7vZ","outputId":"a33eac3d-c32c-49dd-80db-63a821cd3b3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# mount google drive\n","from google.colab import drive\n","# this resets all imported csvs\n","drive.flush_and_unmount()\n","# mount/remount\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":155,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1745257737908,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"hhq7-3bAd7s5"},"outputs":[],"source":["# library imports\n","\n","# data handling and calcs\n","import pandas as pd\n","# calculations and numbers\n","import numpy as np\n","# file handling:\n","import os\n","# hyperparameter search for validation fold\n","# (created a custom search grid function instead)\n","#from sklearn.model_selection import GridSearchCV\n","# creation of combination dict for validation fold\n","from itertools import product\n","# scoring validation fold:\n","# (validation fold benchmarked by overall accuracy)\n","from sklearn.metrics import accuracy_score\n","# clone is necessitated by the validation loop:\n","# (need to reset model back to base after fitting with each param combo)\n","from sklearn.base import clone\n","# using csv to extract headers only from csv file\n","import csv\n","# for file name incrementing\n","from datetime import datetime\n","# supress filters\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":156,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1745257737912,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"yjhTiRaCd7qa"},"outputs":[],"source":["# model selection library imports\n","\n","# Random Forest Classifier\n","from sklearn.ensemble import RandomForestClassifier\n","# KNN\n","from sklearn.neighbors import KNeighborsClassifier\n","# Niave Bayes\n","from sklearn.naive_bayes import GaussianNB\n","# MLP Classifier\n","from sklearn.neural_network import MLPClassifier\n","\n"]},{"cell_type":"code","execution_count":157,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1745257737933,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"9OmjyUEwd7oB"},"outputs":[],"source":["# fold selection cell\n","\n","# define shared drive location\n","shared_drive = r\"/content/drive/MyDrive/Capstone_Docs_Shared\"\n","\n","active_fold = \"1-Train\"\n","#active_fold = \"2-Validate\"\n","#active_fold = \"3-Test\"\n","\n","# use engineered features for train fold?\n","engineered_features = True\n","#engineered_features = False\n","\n","# (Optional: ticker list subset)\n","# (running them all takes a long time...\n","# ...if you know which ones you would like to filter...\n","# ...you can choose those now)\n","#selected_ticker_list = []\n","\n","# define desired model for this NB run:\n","#selected_model = \"RandomForestClass\"\n","selected_model = \"KNN\"\n","#selected_model = \"NaiveBayesGauss\"\n","#selected_model = \"DecisionTree\"\n","#selected_model = \"MLP\"\n","\n","# define desired prediction value:\n","predicted_feature = 'D_5_Label'\n","# add more here if desired\n","\n"]},{"cell_type":"code","execution_count":158,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1745257737943,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"O5y5dGWmd7lq"},"outputs":[],"source":["# store model configurations for grid search in validation fold\n","# (if no selections, use defaults...)\n","# (note: this is a subselection of parameter options chosen after...\n","# ... some experimentation to make the project demo NB run faster...\n","# ... older, wider, parameter selections have been discarded)\n","\n","model_config_dict = {\n","    # user selection definition\n","    \"RandomForestClass\" : {\n","        # actual model from map\n","        \"model\" : RandomForestClassifier(random_state=42),\n","        # add parameters to tune\n","        \"params\" : {\n","            \"n_estimators\" : [5, 50,],\n","            \"max_depth\" : [10, 15,],\n","            \"class_weight\" : [\"balanced\", \"balanced_subsample\"],\n","            #\"max_features\" : [\"sqrt\", \"log2\", None],\n","            #\"min_samples_split\" : [2, 5, 10],\n","            #\"min_samples_leaf\" : [1, 2, 4],\n","        }\n","    },\n","    # next user selection definition\n","    \"KNN\" : {\n","        \"model\" : KNeighborsClassifier(),\n","        \"params\" : {\n","            \"n_neighbors\" : [18, 21, 24],\n","            \"weights\" : [\"uniform\", \"distance\"],\n","            #\"weights\" : [\"uniform\"],\n","            #\"algorithm\" : [\"auto\", \"ball_tree\"],\n","            \"leaf_size\" : [10, 30],\n","            #\"p\" : [1, 2],\n","        }\n","    },\n","    # thid user selection definition\n","    \"NaiveBayesGauss\" : {\n","        \"model\" : GaussianNB(),\n","        \"params\" : {\n","            \"var_smoothing\" : [1e-9, 1e-8, 1e-7, 1e-6, 1e-5],\n","        }\n","    },\n","    # user selection:\n","    \"MLP\" : {\n","        \"model\" : MLPClassifier(random_state=42),\n","        \"params\" : {\n","            \"hidden_layer_sizes\" : [(50, 50), (100, 100)],\n","            \"activation\" : [\"relu\"],\n","            \"solver\" : [\"adam\"],\n","            \"alpha\" : [0.0001, 0.001, 0.01],\n","            \"learning_rate\" : [\"constant\", \"adaptive\"],\n","            \"max_iter\" : [100, 200],\n","            \"batch_size\" : [32, 16, 8],\n","            \"early_stopping\" : [True],\n","        }\n","    }\n","}\n","\n","\n"]},{"cell_type":"code","execution_count":159,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1745257737948,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"zjPQhxwjd7jR"},"outputs":[],"source":["# Load data using fold selection\n","\n","def load_data(fold_selection, use_engineered_features, shared_loc):\n","\n","  # ID filenames based on fold selection and flags\n","  if fold_selection == \"1-Train\":\n","    # if flag for engineered features = True\n","    if use_engineered_features:\n","      train_data_name = \"train_fold_1_fe.csv\"\n","      test_data_name = \"test_fold_1_fe.csv\"\n","      print(\"Selected: Fold 1, Engineered Features\")\n","    else:\n","      train_data_name = \"train_fold_1.csv\"\n","      test_data_name = \"test_fold_1.csv\"\n","      print(\"Selected: Fold 1, All Features\")\n","  elif fold_selection == \"2-Validate\":\n","    train_data_name = \"train_fold_2.csv\"\n","    test_data_name = \"test_fold_2.csv\"\n","    print(\"Selected: Fold 2\")\n","  elif fold_selection == \"3-Test\":\n","    train_data_name = \"train_fold_3.csv\"\n","    test_data_name = \"test_fold_3.csv\"\n","    print(\"Selected: Fold 3\")\n","  else:\n","    print(\"Invalid fold selection\")\n","\n","  # Based on selected filenames, assemble train/test split filepaths\n","  train_data_path = os.path.join(shared_loc, train_data_name)\n","  test_data_path = os.path.join(shared_loc, test_data_name)\n","  print(\"Data paths assembled\")\n","  print(train_data_path)\n","  print(test_data_path)\n","\n","  # make acutal data load based on assembled path\n","  train_df = pd.read_csv(train_data_path)\n","  test_df = pd.read_csv(test_data_path)\n","  print(\"Data load complete\")\n","  print(\"\")\n","  print('FOLD DATA STATS:')\n","  print(\"Training Split Null Values:\")\n","  print(train_df.isna().sum().sum())\n","  print(\"Testing Split Null Values:\")\n","  print(test_df.isna().sum().sum())\n","  print(\"\")\n","  print(\"Column Check:\")\n","  print(train_df.columns)\n","  print(test_df.columns)\n","\n","\n","  return train_df, test_df\n"]},{"cell_type":"code","source":["from re import sub\n","# Function to check for duplicate days\n","\n","def check_duplicate_days(input_df):\n","  # take a copy\n","  df = input_df.copy()\n","  # drop duplicate days from df\n","  dupes = df.duplicated(subset=['Date','Ticker'], keep='first')\n","  dup_count = dupes.sum()\n","  if dup_count > 0:\n","    df = df.drop_duplicates(subset=['Date','Ticker'], keep='first')\n","  return df\n","\n","\n"],"metadata":{"id":"688ko9vgVwmg","executionInfo":{"status":"ok","timestamp":1745257737951,"user_tz":240,"elapsed":1,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"}}},"execution_count":160,"outputs":[]},{"cell_type":"code","execution_count":161,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1745257737961,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"w3m6Mkz1d7hB"},"outputs":[],"source":["# Function to check for Unnamed Col\n","# (This sometimes happens as dfs are saved to csv)\n","# (Including this in case instructional team wants to mess around with inputs)\n","\n","def check_for_unnamed_cols(input_df):\n","  # take a copy\n","  df = input_df.copy()\n","  # ID unnamed cols:\n","  unnamed_cols = [col for col in df.columns if 'Unnamed' in col]\n","  # remove unnamed cols\n","  df = df.drop(columns=unnamed_cols)\n","  clean_df = df\n","  print(\"Unnamed cols removed\")\n","  return clean_df\n"]},{"cell_type":"code","execution_count":162,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1745257737989,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"1XRtpub7hI8X"},"outputs":[],"source":["# function to assemble and track a filtered list of tickers while modeling\n","\n","def assemble_ticker_list(input_df, num_gens):\n","  df = input_df.copy()\n","  #print(df)\n","  # split tickers amongst selected subslputs\n","  ticker_lists = np.array_split(df['Ticker'].unique(), num_gens)\n","\n","  return ticker_lists"]},{"cell_type":"code","execution_count":163,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1745257737991,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"CYy8pSr9hI5v"},"outputs":[],"source":["# function to filter input df by ticker\n","\n","def filter_by_ticker(input_df, ticker_lst):\n","  df = input_df.copy()\n","  df = df[df['Ticker'].isin(ticker_lst)]\n","  # remember to reindex:\n","  df = df.reset_index(drop=True)\n","  print(\"Number of Tickers In Selected Data Split:\")\n","  print(df['Ticker'].nunique())\n","  print(\"Number of Days In Input Df:\")\n","  print(len(df) // df['Ticker'].nunique())\n","  return df\n","\n"]},{"cell_type":"code","execution_count":164,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1745257737992,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"PADhpfW6hI3H"},"outputs":[],"source":["# function to remove unused prediction columns from dfs\n","def remove_unused_labels(input_df, pred_col):\n","  df = input_df.copy()\n","  # if column has \"label\" in text, remove, ...\n","  # except if pred_col\n","  drop_cols = [col for col in df.columns if 'Label' in col and col != pred_col]\n","  df = df.drop(columns=drop_cols)\n","  # also drop close column as this is a predictive column\n","  drop_cols = [col for col in df.columns if 'Close' in col and col != pred_col]\n","  df = df.drop(columns=drop_cols)\n","\n","  return df"]},{"cell_type":"code","execution_count":165,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1745257738013,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"ip6bLjv9hI0v"},"outputs":[],"source":["# function to ID 'Offset' Columns, remove thier non-offset counterparts...\n","# (necessary to prevent data leakage)\n","\n","def remove_leakage_vars(input_df):\n","  df = input_df.copy()\n","  # find Offset Columns\n","  offset_cols = [col for col in df.columns if '_Offset' in col]\n","  # find non-offset counterparts\n","  non_offset_cols = [col.replace('_Offset', '') for col in offset_cols]\n","  # drop non-offset columns\n","  df = df.drop(columns=non_offset_cols)\n","  print(df.columns)\n","  return df"]},{"cell_type":"code","execution_count":166,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1745257738014,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"I4vzDX6RhIyf"},"outputs":[],"source":["# function to filter val dataset and test dataset...\n","# ... based on columns present in F.E. Trian dataset\n","def filter_fe_cols(input_df, fold_select):\n","  df = input_df.copy()\n","\n","  if fold_select == \"1-Train\":\n","    # determine fe file name (train selection, feature extracted)\n","    df = df\n","  elif fold_select == \"2-Validate\" or \"3_Train\":\n","    # determine fe file name (train selection, feature extracted)\n","    filtered_col_filename = \"train_fold_1_fe.csv\"\n","    # make filepath based on name\n","    filtered_col_filepath = os.path.join(shared_drive, filtered_col_filename)\n","    # read only the first row (header) of columns source file\n","    with open(filtered_col_filepath, newline='') as csvfile:\n","      reader = csv.reader(csvfile)\n","      first_row = next(reader)\n","    # apply filter to df:\n","    df = df[first_row]\n","\n","  return df"]},{"cell_type":"code","execution_count":167,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1745257738028,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"k-oJw49fhIwG"},"outputs":[],"source":["# function for validation stage custom parameter search:\n","def custom_param_search(base_model, params_options, train_set, test_set, pred_feat):\n","\n","  # filter down to even less tickers for sake of time:\n","  # ##### Do that here #####\n","  # (even 1 ticker technically provides like 200 tests per run)\n","  u_tickers = train_set['Ticker'].unique()\n","\n","  # take first ticker from list:\n","  one_ticker = u_tickers[[0]]\n","  # filter train and test set by selected ticker\n","  train_set = train_set[train_set['Ticker'].isin(one_ticker)]\n","  test_set = test_set[test_set['Ticker'].isin(one_ticker)]\n","\n","  # log best score =\n","  best_score = -1\n","  # log  best params\n","  best_params = None\n","  # log results of each param combo:\n","  param_scores = []\n","\n","  # A. assemble param combos based on input grid:\n","  # get key , value parings first:\n","  keys, vals = zip(*params_options.items())\n","  # assemble dict obj from k/v paris\n","  param_combos = [dict(zip(keys, v)) for v in product(*vals)]\n","\n","  print(f\"Number of Parameter Combinations for {base_model}: \", len(param_combos))\n","\n","  # Replicate signal prediction function here, except with each param combo:\n","\n","  # initialize list to store predicted signals for each ticker\n","  ticker_predicted_signals = []\n","  # initialize list to store predicted prices for all tickers\n","  all_predicted_signals = []\n","  # take a copy of the input dfs\n","  train_data = train_set.copy()\n","  test_data = test_set.copy()\n","\n","  # define X, y, and tracking features\n","\n","  # tracking features (exclude from model, re-merge later)\n","  analysis_feats = ['Date', 'Ticker']\n","  # remove analysis features...\n","  filtered_features = [x for x in train_data.columns if x not in analysis_feats]\n","  # remove predicted features...\n","  # ... and you are left with X features\n","  model_X_feats = [x for x in filtered_features if x != pred_feat]\n","\n","  # B. initialize loop to iterate over tickers\n","  for ticker_symbol in train_data['Ticker'].unique():\n","    print(\"Ticker Selected for Validation: \", ticker_symbol)\n","    # Filter input dfs to isolate ticker[i] time series\n","    ticker_train_df = train_data[train_data['Ticker'] == ticker_symbol]\n","    ticker_test_df = test_data[test_data['Ticker'] == ticker_symbol]\n","    # remember to reindex\n","    ticker_train_df = ticker_train_df.reset_index(drop=True)\n","    ticker_test_df = ticker_test_df.reset_index(drop=True)\n","    # filter X by selected features\n","    train_data_X = ticker_train_df[model_X_feats]\n","    test_data_X = ticker_test_df[model_X_feats]\n","    # filter y by predicted feature\n","    train_data_y = ticker_train_df[pred_feat]\n","    test_data_y = ticker_test_df[pred_feat]\n","\n","    # C. loop over parameter combinations:\n","    for params in param_combos:\n","      print(\"--------------- New Parameter Selection Initiated ---------------\")\n","      print(\"Parameter Combination Selected: \")\n","      print(params)\n","      # track pred vs actual at NB lvl\n","      # unfortunately need to do evaluation in the modeling notebook for validation stage to save time, memory, files, etc.\n","      preds = []\n","      acts = []\n","\n","      # need to use clone here...\n","      # Resets model back to clean slate after previous param combo load\n","      model = clone(base_model).set_params(**params)\n","\n","      # replicate rolling training set\n","      extended_train_data_X = train_data_X.copy()\n","      extended_train_data_y = train_data_y.copy()\n","\n","      # replicate daily looping\n","      for eval_day in range(len(test_data_X)):\n","        # replicate x and y slicing\n","        curr_test_X = test_data_X.iloc[[eval_day]]\n","        curr_test_y = test_data_y.iloc[eval_day]\n","        # fit on current training slice:\n","        model.fit(extended_train_data_X, extended_train_data_y)\n","        # predict on current test slice:\n","        y_pred = model.predict(curr_test_X)[0]\n","        # (try removing from list here^^^)\n","        # append day's prediction to list\n","        preds.append(y_pred)\n","        # append day's actual to list\n","        acts.append(curr_test_y)\n","        # replicate extending training set to overlap new day\n","        extended_train_data_X = pd.concat([extended_train_data_X, curr_test_X], ignore_index=True)\n","        extended_train_data_y = pd.concat([extended_train_data_y, pd.Series([curr_test_y])], ignore_index=True)\n","\n","      # Scoring -- use as basis of results tracking\n","      score = accuracy_score(acts, preds)\n","      param_scores.append((params, score))\n","      print(\"Score Metric for Selected Combo:\", score)\n","\n","      # update best scores and parameters, given logged score\n","      if score > best_score:\n","        best_score = score\n","        best_params = params\n","        print(\"New Best Score: \", best_score)\n","        print(\"New Best Parameters: \", best_params)\n","\n","  print(\"Model HP Tuning Complete! YeeHaw\")\n","  return best_params, param_scores\n"]},{"cell_type":"code","execution_count":168,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1745257738034,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"Jo4yYmYahIt5"},"outputs":[],"source":["# function to determine model parameter selection:\n","def parameter_selection(model_name, model_config_dict, fold_selection, train_set, test_set, pred_feat):\n","  # determine what parameters to select based on active fold\n","  if fold_selection == \"1-Train\":\n","    print(\"Selected: Fold 1\")\n","    params_to_use = None\n","  elif fold_selection == \"2-Validate\":\n","    # pass dict values from model selection here\n","    print(\"Selected: Fold 2\")\n","    # determine base model type\n","    val_model = model_config_dict[model_name]['model']\n","    # determine parameter grid for given model type\n","    params_options = model_config_dict[model_name]['params']\n","    # typical next step, set up grid-search grid object\n","    # (BUT.. grid search doesn't really work with looping modeling/expanding seen data)\n","    # (need to set up custom grid search instead:)\n","    best_params, param_scores = custom_param_search(val_model, params_options, train_set, test_set, pred_feat)\n","    params_to_use = best_params\n","\n","  elif fold_selection == \"3-Test\":\n","    # Load In --- best parameters as determined by Val fold\n","    print(\"Selected: Fold 3\")\n","    # load parameter set from best_params doc and convert to dict for test modeling\n","    file_name = f'best_params_{model_name}_val.csv'\n","    file_path = os.path.join(shared_drive, file_name)\n","    param_df = pd.read_csv(file_path)\n","    # (this is importing ints as strings, because there is a mixture of...\n","    # data types in the same col... sends all to str)\n","\n","    # helper function to convert datatypes:\n","    def change_type(val):\n","      try:\n","        return eval(val)\n","      except:\n","          return val\n","\n","    param_dict = {\n","        row['params']: change_type(row['selection_values']) for _, row in param_df.iterrows()\n","    }\n","    print(param_dict)\n","    params_to_use = param_dict\n","\n","  else:\n","    print(\"Invalid fold selection\")\n","\n","  return params_to_use\n"]},{"cell_type":"code","execution_count":169,"metadata":{"executionInfo":{"elapsed":31,"status":"ok","timestamp":1745257738060,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"IUTNgOf_hIre"},"outputs":[],"source":["# function to run selected modeling scenario:\n","# (this model is focused on direct signal prediction, ...\n","# rather than point price prediction and translation to signals)\n","\n","def daily_signal_prediction(model_name, model_detail_dict, parameters, train_split, test_split, predict_feat, fold_selection, use_engineered_features):\n","  # optional code included to log multi-ticker (based on unique tickers passed thru train_split)\n","  # OR (project demo version -->) one ticker at a time (len(u_t)=1)\n","  # (i.e. if you have more processing power, you can run this with many tickers in seen/unseen_splits)\n","\n","  # initialize list to store predicted signals for each ticker\n","  ticker_predicted_signals = []\n","  # initialize list to store predicted prices for all tickers\n","  all_predicted_signals = []\n","  # take a copy of the input dfs\n","  train_data = train_split.copy()\n","  test_data = test_split.copy()\n","  # end df :\n","  pred_df = None\n","\n","  # define X, y, and tracking features\n","\n","  # tracking features (exclude from model, re-merge later)\n","  analysis_feats = ['Date', 'Ticker']\n","  # remove analysis features...\n","  filtered_features = [x for x in train_data.columns if x not in analysis_feats]\n","  # remove predicted features...\n","  # ... and you are left with X features\n","  model_X_feats = [x for x in filtered_features if x != predict_feat]\n","\n","  # initialize loop to iterate over tickers\n","  for ticker_symbol in train_data['Ticker'].unique():\n","    # Filter input dfs to isolate ticker[i] time series\n","    ticker_train_df = train_data[train_data['Ticker'] == ticker_symbol]\n","    ticker_test_df = test_data[test_data['Ticker'] == ticker_symbol]\n","    # remember to reindex\n","    ticker_train_df = ticker_train_df.reset_index(drop=True)\n","    ticker_test_df = ticker_test_df.reset_index(drop=True)\n","    # filter X by selected features\n","    train_data_X = ticker_train_df[model_X_feats]\n","    test_data_X = ticker_test_df[model_X_feats]\n","    # filter y by predicted feature\n","    train_data_y = ticker_train_df[predict_feat]\n","    test_data_y = ticker_test_df[predict_feat]\n","\n","    # iterate through days...\n","    # ... make prediction on every day of unseen df...\n","    # ... using a growing training set\n","\n","    # initialize growing training set, forked from input training set\n","    extended_train_data_X = train_data_X.copy()\n","    # confirm all column names are string formatted\n","    extended_train_data_X.columns = extended_train_data_X.columns.astype(str)\n","    extended_train_data_y = train_data_y.copy()\n","\n","    # initialize loop to iterate over testing days\n","    for eval_day in range(len(ticker_test_df)):\n","      # Split Rolling Train/Test Sets:\n","      # (going by index, so no need to define date ranges)\n","      # take current X test slice:\n","      # (turning this into a mini df makes extension easier later)\n","      curr_test_X = test_data_X.iloc[[eval_day]]\n","      # confirm all column names are string formatted\n","      curr_test_X.columns = curr_test_X.columns.astype(str)\n","      # take current y test slice\n","      curr_test_y = test_data_y.iloc[eval_day]\n","      # (c_t_y = act_label)\n","      # pull date based on index for logging:\n","      curr_test_date = ticker_test_df.iloc[eval_day]['Date']\n","\n","      # Training / Fitting Model Stage:\n","      # (fork here for train, val, test)\n","      # (train, train_s_f, and test should all operate similarly...)\n","      # (train uses default params)\n","      # (val uses grid search cv)\n","      # (test should use params selected in val)\n","\n","      if fold_selection == \"1-Train\":\n","\n","        if use_engineered_features:\n","          # fit model with all default parameters to gauge f.e. performance\n","          model = model_detail_dict[model_name]['model']\n","          model.fit(extended_train_data_X, extended_train_data_y)\n","        else:\n","          # fit model with all default parameters to gauge baseline performance\n","          model = model_detail_dict[model_name]['model']\n","          model.fit(extended_train_data_X, extended_train_data_y)\n","\n","        # Predict selected y_feat on Nth Day:\n","        y_pred = model.predict(curr_test_X)\n","        # Append predictions to results list:\n","        ticker_predicted_signals.append((curr_test_date, ticker_symbol, y_pred[0], curr_test_y))\n","        # extend training set to overlap most recent test\n","        extended_train_data_X = pd.concat([extended_train_data_X, curr_test_X], ignore_index=True)\n","        # keep track of running actual ys\n","        extended_train_data_y = pd.concat([extended_train_data_y, pd.Series([curr_test_y])], ignore_index=True)\n","        # (don't actually use, but in there for tracking and running results visualization)\n","        # confirm all column names are string formatted\n","        extended_train_data_X.columns = extended_train_data_X.columns.astype(str)\n","\n","      elif fold_selection == \"2-Validate\":\n","\n","        # take parameters generated by validation loop\n","        base_model = model_detail_dict[model_name]['model']\n","        ideal_parameters = parameters\n","        # (best parameters are already generated in..\n","        #... parameter_selection(custom_ranking())...\n","        # passed here thru 'parameters' input)\n","        model = base_model.set_params(**ideal_parameters)\n","        #print(model)\n","        model.fit(extended_train_data_X, extended_train_data_y)\n","        # Predict selected y_feat on Nth Day:\n","        y_pred = model.predict(curr_test_X)\n","        # Append predictions to results list:\n","        ticker_predicted_signals.append((curr_test_date, ticker_symbol, y_pred[0], curr_test_y))\n","        # extend training set to overlap most recent test\n","        extended_train_data_X = pd.concat([extended_train_data_X, curr_test_X], ignore_index=True)\n","        # keep track of running actual ys\n","        extended_train_data_y = pd.concat([extended_train_data_y, pd.Series([curr_test_y])], ignore_index=True)\n","        # (don't actually use, but in there for tracking and running results visualization)\n","        # confirm all column names are string formatted\n","        extended_train_data_X.columns = extended_train_data_X.columns.astype(str)\n","\n","      elif fold_selection == \"3-Test\":\n","\n","        #print(\"Modeling: Fold 3, Testing Fold\")\n","        base_model = model_detail_dict[model_name]['model']\n","        # use parameters passed from fold 2\n","        validated_parameters = parameters\n","        model = base_model.set_params(**validated_parameters)\n","        model.fit(extended_train_data_X, extended_train_data_y)\n","        y_pred = model.predict(curr_test_X)\n","        ticker_predicted_signals.append((curr_test_date, ticker_symbol, y_pred[0], curr_test_y))\n","        extended_train_data_X = pd.concat([extended_train_data_X, curr_test_X], ignore_index=True)\n","        extended_train_data_y = pd.concat([extended_train_data_y, pd.Series([curr_test_y])], ignore_index=True)\n","        extended_train_data_X.columns = extended_train_data_X.columns.astype(str)\n","\n","      else:\n","        print(\"Invalid fold selection\")\n","\n","    # extend results to output df\n","    all_predicted_signals.extend(ticker_predicted_signals)\n","    print(\"TICKER COMPLETED: \", ticker_symbol)\n","\n","  # convert predictions to df\n","  pred_df = pd.DataFrame(all_predicted_signals, columns=[\"Date\",\"Ticker\",f\"Predicted_Signal_{predict_feat}\",f\"Actual_Signal{predict_feat}\"])\n","\n","  return pred_df\n","\n"]},{"cell_type":"code","execution_count":170,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11861,"status":"ok","timestamp":1745257749920,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"5kkX6sFFhIo3","outputId":"96f9dce8-98e9-4aa0-b0fb-2eeeb7b6f17e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Selected: Fold 1, Engineered Features\n","Data paths assembled\n","/content/drive/MyDrive/Capstone_Docs_Shared/train_fold_1_fe.csv\n","/content/drive/MyDrive/Capstone_Docs_Shared/test_fold_1_fe.csv\n","Data load complete\n","\n","FOLD DATA STATS:\n","Training Split Null Values:\n","0\n","Testing Split Null Values:\n","0\n","\n","Column Check:\n","Index(['Date', 'Ticker', 'Close', 'D_Return', 'W_Return', 'M_Return',\n","       'D_Return_Offset', 'W_Return_Offset', 'M_Return_Offset', 'D_2_Label',\n","       'W_2_Label', 'M_2_Label', 'D_3_Label', 'W_3_Label', 'M_3_Label',\n","       'D_5_Label', 'W_5_Label', 'M_5_Label', 'Open', 'High', 'Low', 'Volume',\n","       'UMCSENT', 'HOUST', 'RSXFS', 'ICSA', 'T10Y2Y', 'PPIACO', 'ADXDNO',\n","       'Year', 'RSI_5', 'RSI_7', 'RSI_14', 'RSI_21', 'RSI_30', 'RSI_50',\n","       'Bollinger_Lower_10', 'Bollinger_Upper_100', 'Bollinger_Lower_100',\n","       'Stoch_%K_5', 'Stoch_%K_7', 'Stoch_%K_14', 'Stoch_%K_30', 'ADX_7',\n","       'ADX_14', 'ADX_20', 'ADX_30', 'ADX_50', 'ROC_7', 'ROC_14', 'ROC_30',\n","       'Williams_R_[14]', 'Ichimoku_B', 'Open_Offset', 'Low_Offset',\n","       'High_Offset', 'Close_Offset', 'Volume_Offset'],\n","      dtype='object')\n","Index(['Date', 'Ticker', 'Close', 'D_Return', 'W_Return', 'M_Return',\n","       'D_Return_Offset', 'W_Return_Offset', 'M_Return_Offset', 'D_2_Label',\n","       'W_2_Label', 'M_2_Label', 'D_3_Label', 'W_3_Label', 'M_3_Label',\n","       'D_5_Label', 'W_5_Label', 'M_5_Label', 'Open', 'High', 'Low', 'Volume',\n","       'UMCSENT', 'HOUST', 'RSXFS', 'ICSA', 'T10Y2Y', 'PPIACO', 'ADXDNO',\n","       'Year', 'RSI_5', 'RSI_7', 'RSI_14', 'RSI_21', 'RSI_30', 'RSI_50',\n","       'Bollinger_Lower_10', 'Bollinger_Upper_100', 'Bollinger_Lower_100',\n","       'Stoch_%K_5', 'Stoch_%K_7', 'Stoch_%K_14', 'Stoch_%K_30', 'ADX_7',\n","       'ADX_14', 'ADX_20', 'ADX_30', 'ADX_50', 'ROC_7', 'ROC_14', 'ROC_30',\n","       'Williams_R_[14]', 'Ichimoku_B', 'Open_Offset', 'Low_Offset',\n","       'High_Offset', 'Close_Offset', 'Volume_Offset'],\n","      dtype='object')\n"]}],"source":["# execute Train/Test Split based on Fold selection:\n","# AKA: load Seen/Unseen Split based on Fold selection:\n","fold_train_df, fold_test_df = load_data(active_fold, engineered_features, shared_drive)\n","# (should have no null values a this point)\n","# (column check allows user to view imported data columns)"]},{"cell_type":"code","execution_count":171,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1236,"status":"ok","timestamp":1745257751154,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"gZDSpnfYhImQ","outputId":"ef557956-72a7-4999-dd8b-a6ccfe9445ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unnamed cols removed\n","Unnamed cols removed\n","Number of Tickers In Seen Data:\n","676\n","Tickers Selected For Modeling: \n","['AA' 'RF' 'BRO' 'HOG' 'SCI']\n","Number of Tickers In Selected Data Split:\n","5\n","Number of Days In Input Df:\n","411\n","Number of Tickers In Selected Data Split:\n","5\n","Number of Days In Input Df:\n","411\n","Index(['Date', 'Ticker', 'D_Return_Offset', 'W_Return_Offset',\n","       'M_Return_Offset', 'D_5_Label', 'UMCSENT', 'HOUST', 'RSXFS', 'ICSA',\n","       'T10Y2Y', 'PPIACO', 'ADXDNO', 'Year', 'RSI_5', 'RSI_7', 'RSI_14',\n","       'RSI_21', 'RSI_30', 'RSI_50', 'Bollinger_Lower_10',\n","       'Bollinger_Upper_100', 'Bollinger_Lower_100', 'Stoch_%K_5',\n","       'Stoch_%K_7', 'Stoch_%K_14', 'Stoch_%K_30', 'ADX_7', 'ADX_14', 'ADX_20',\n","       'ADX_30', 'ADX_50', 'ROC_7', 'ROC_14', 'ROC_30', 'Williams_R_[14]',\n","       'Ichimoku_B', 'Open_Offset', 'Low_Offset', 'High_Offset',\n","       'Volume_Offset'],\n","      dtype='object')\n","Index(['Date', 'Ticker', 'D_Return_Offset', 'W_Return_Offset',\n","       'M_Return_Offset', 'D_5_Label', 'UMCSENT', 'HOUST', 'RSXFS', 'ICSA',\n","       'T10Y2Y', 'PPIACO', 'ADXDNO', 'Year', 'RSI_5', 'RSI_7', 'RSI_14',\n","       'RSI_21', 'RSI_30', 'RSI_50', 'Bollinger_Lower_10',\n","       'Bollinger_Upper_100', 'Bollinger_Lower_100', 'Stoch_%K_5',\n","       'Stoch_%K_7', 'Stoch_%K_14', 'Stoch_%K_30', 'ADX_7', 'ADX_14', 'ADX_20',\n","       'ADX_30', 'ADX_50', 'ROC_7', 'ROC_14', 'ROC_30', 'Williams_R_[14]',\n","       'Ichimoku_B', 'Open_Offset', 'Low_Offset', 'High_Offset',\n","       'Volume_Offset'],\n","      dtype='object')\n","Index(['Date', 'Ticker', 'D_Return_Offset', 'W_Return_Offset',\n","       'M_Return_Offset', 'D_5_Label', 'UMCSENT', 'HOUST', 'RSXFS', 'ICSA',\n","       'T10Y2Y', 'PPIACO', 'ADXDNO', 'Year', 'RSI_5', 'RSI_7', 'RSI_14',\n","       'RSI_21', 'RSI_30', 'RSI_50', 'Bollinger_Lower_10',\n","       'Bollinger_Upper_100', 'Bollinger_Lower_100', 'Stoch_%K_5',\n","       'Stoch_%K_7', 'Stoch_%K_14', 'Stoch_%K_30', 'ADX_7', 'ADX_14', 'ADX_20',\n","       'ADX_30', 'ADX_50', 'ROC_7', 'ROC_14', 'ROC_30', 'Williams_R_[14]',\n","       'Ichimoku_B', 'Open_Offset', 'Low_Offset', 'High_Offset',\n","       'Volume_Offset'],\n","      dtype='object')\n"]}],"source":["# make copies of imported splits\n","train_df = fold_train_df.copy()\n","test_df = fold_test_df.copy()\n","\n","# check for duplicate days:\n","train_df = check_duplicate_days(train_df)\n","test_df = check_duplicate_days(test_df)\n","\n","# filter by feature extracted features:\n","train_df = filter_fe_cols(train_df, active_fold)\n","test_df = filter_fe_cols(test_df, active_fold)\n","\n","# check for unnamed columns\n","train_df = check_for_unnamed_cols(train_df)\n","test_df = check_for_unnamed_cols(test_df)\n","\n","# print unique tickers from train df\n","print(\"Number of Tickers In Seen Data:\")\n","print(train_df['Ticker'].nunique())\n","\n","# assemble lists to split tickers into more managable sections:\n","ticker_lists = assemble_ticker_list(train_df, 10)\n","# (subselection carries through to later docs)\n","# Take a smaller subsample\n","selected_ticker_list = ticker_lists[0]\n","#print(ticker_list)\n","# In the interest of time, take an even smaller subsample\n","subselected_ticker_list = selected_ticker_list[:5]\n","print(\"Tickers Selected For Modeling: \")\n","print(subselected_ticker_list)\n","\n","# filter input by selected tickers:\n","#train_df = filter_by_ticker(train_df, selected_ticker_list)\n","train_df = filter_by_ticker(train_df, subselected_ticker_list)\n","#test_df = filter_by_ticker(test_df, selected_ticker_list)\n","test_df = filter_by_ticker(test_df, subselected_ticker_list)\n","\n","# need to remove non-utilized target columns from df input:\n","train_df = remove_unused_labels(train_df, predicted_feature)\n","test_df = remove_unused_labels(test_df, predicted_feature)\n","\n","# ID offset columns, remove their non-offset counterparts before predictions:\n","train_df = remove_leakage_vars(train_df)\n","test_df = remove_leakage_vars(test_df)\n","\n","# pre-modeling column check:\n","print(train_df.columns)\n","\n"]},{"cell_type":"code","execution_count":172,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1745257751161,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"MpGatIwOn1Lc","outputId":"4099f392-3b2a-4868-d9c3-f6d56f599806"},"outputs":[{"output_type":"stream","name":"stdout","text":["Selected: Fold 1\n"]}],"source":["# determine parameters to use based on fold selection\n","parameters = parameter_selection(selected_model, model_config_dict, active_fold, train_df, test_df, predicted_feature)\n","\n","# if validation operations in fold 2...\n","# ...convert best params and scores to df for export\n","if active_fold == \"2-Validate\":\n","  param_scores_df = pd.DataFrame([\n","      {'params':str(p), 'selection_values':s} for p, s in parameters.items()\n","  ])\n","  print(param_scores_df)\n","  # assemble filename for best params export\n","  best_params_filename = f'best_params_{selected_model}_val.csv'\n","  # assemble filepath for best params export\n","  best_params_filepath = os.path.join(shared_drive, best_params_filename)\n","  # save best params to csv\n","  param_scores_df.to_csv(best_params_filepath, index=False)\n","  print(\"Validation Best Parameters File Generated\")\n"]},{"cell_type":"code","execution_count":173,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1745257751176,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"LL1EqFLAoHZG"},"outputs":[],"source":["# function to make an output path\n","\n","def assemble_output_path(fold_selection, use_engineered_features, shared_drive, sel_parameters, model_str, pred_feat_str):\n","  # ID filenames based on fold selection and flags\n","  if fold_selection == \"1-Train\":\n","    # if flag for engineered features = True\n","    if use_engineered_features:\n","      # assemble filename:\n","      output_filename = f\"predictions_{pred_feat_str}_{model_str}_fold_1_fe.csv\"\n","      print(\"Output File Generating: Fold 1, Engineered Features\")\n","    else:\n","      # assemble filename:\n","      output_filename = f\"predictions_{pred_feat_str}_{model_str}_fold_1_all.csv\"\n","      print(\"Output File Generating: Fold 1, All Features\")\n","\n","  elif fold_selection == \"2-Validate\":\n","    # assemble filename\n","    output_filename = f\"predictions_{pred_feat_str}_{model_str}_fold_2.csv\"\n","    print(\"Output File Generating: Fold 2\")\n","\n","  elif fold_selection == \"3-Test\":\n","    # assemble filename\n","    # requires only one output, so easy to name based on model and fold\n","    output_filename = f\"predictions_{pred_feat_str}_{model_str}_fold_3.csv\"\n","    print(\"Output File Generating: Fold 3\")\n","\n","  else:\n","    print(\"Invalid fold selection\")\n","\n","  # assemble output filepath:\n","  output_filepath = os.path.join(shared_drive, output_filename)\n","  print(\"Output filepath assembled\")\n","  print(output_filepath)\n","\n","  return output_filepath\n"]},{"cell_type":"code","execution_count":174,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1745257751178,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"lur6fDlhoR8i"},"outputs":[],"source":["# function to run modeling one ticker at a time, push updates to csv as they finish:\n","def iterate_and_export_per_ticker(out_filepath, model_select, model_config_dict_name, parameters, train_df, test_df, pred_feat, fold_selection, use_engineered_features):\n","  # lists of tickers available for export\n","  # (after all previous filtering)\n","  final_ticker_lst = train_df['Ticker'].unique()\n","  print(final_ticker_lst)\n","\n","  print(train_df['Ticker'].value_counts())\n","  print(train_df['Ticker'].duplicated().sum())\n","\n","  print(\"Modeling Initiated for Selected Tickers, Please Wait...\")\n","\n","  # check if file aleady exists:\n","  if os.path.exists(out_filepath):\n","    print(\"Output File Already Exists, Incrementing\")\n","    # increment filename by one & add timestamp:\n","    time = datetime.now().strftime(\"%Y%m%d_%H%M\")\n","    out_filepath = out_filepath.replace(\".csv\", f\"_{time}.csv\")\n","\n","    print(\"New Output Filepath:\")\n","    print(out_filepath)\n","\n","  from collections import Counter\n","  print(Counter(final_ticker_lst))\n","\n","  # flag for first export iteration:\n","  first_iteration = True\n","  # initialize loop to cycle prediction dfs per ticker:\n","  for ticker in final_ticker_lst:\n","    print(f\"Ticker Modeling Initialized: {ticker}\")\n","    print(\"Please wait...\")\n","    # filter input dfs to isolate ticker[i] time series\n","    ticker_train_df = train_df[train_df['Ticker'] == ticker]\n","    ticker_test_df = test_df[test_df['Ticker'] == ticker]\n","    # make predictions using daily_signal_prediction:\n","    pred_df = daily_signal_prediction(model_select, model_config_dict_name, parameters, ticker_train_df, ticker_test_df, pred_feat, active_fold, engineered_features)\n","    # Take pred_df as output w/o merging:\n","    output_df = pred_df\n","\n","    # output merged df to csv -- only headers on 1st batch\n","    output_df.to_csv(out_filepath, mode='a', header=first_iteration, index=False)\n","    print(f\"Ticker Export Complete: {ticker}\")\n","    # reset flag\n","    first_iteration = False\n","\n","  return None\n"]},{"cell_type":"code","execution_count":175,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1745257751228,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"},"user_tz":240},"id":"hnkg3AyhoovO","outputId":"58a6b13d-5e01-4ae2-9440-ec0808c8a7d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Output File Generating: Fold 1, Engineered Features\n","Output filepath assembled\n","/content/drive/MyDrive/Capstone_Docs_Shared/predictions_D5Label_KNN_fold_1_fe.csv\n"]}],"source":["# if predicted feature contains underscore:\n","# need to remove for file naming conventions\n","feat_abb = None\n","if \"_\" in predicted_feature:\n","  feat_abb = predicted_feature.replace(\"_\",\"\")\n","# determine where to store prediction results\n","output_location = assemble_output_path(active_fold, engineered_features, shared_drive, parameters, selected_model, feat_abb)"]},{"cell_type":"code","execution_count":176,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b6pt_TNsoron","outputId":"1fe81b5b-ee90-4899-d745-72da93c20bf7","executionInfo":{"status":"ok","timestamp":1745257760487,"user_tz":240,"elapsed":9296,"user":{"displayName":"Joseph Klejna","userId":"02050111539470463240"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['AA' 'RF' 'BRO' 'HOG' 'SCI']\n","Ticker\n","AA     411\n","RF     411\n","BRO    411\n","HOG    411\n","SCI    411\n","Name: count, dtype: int64\n","2050\n","Modeling Initiated for Selected Tickers, Please Wait...\n","Output File Already Exists, Incrementing\n","New Output Filepath:\n","/content/drive/MyDrive/Capstone_Docs_Shared/predictions_D5Label_KNN_fold_1_fe_20250421_1749.csv\n","Counter({'AA': 1, 'RF': 1, 'BRO': 1, 'HOG': 1, 'SCI': 1})\n","Ticker Modeling Initialized: AA\n","Please wait...\n","TICKER COMPLETED:  AA\n","Ticker Export Complete: AA\n","Ticker Modeling Initialized: RF\n","Please wait...\n","TICKER COMPLETED:  RF\n","Ticker Export Complete: RF\n","Ticker Modeling Initialized: BRO\n","Please wait...\n","TICKER COMPLETED:  BRO\n","Ticker Export Complete: BRO\n","Ticker Modeling Initialized: HOG\n","Please wait...\n","TICKER COMPLETED:  HOG\n","Ticker Export Complete: HOG\n","Ticker Modeling Initialized: SCI\n","Please wait...\n","TICKER COMPLETED:  SCI\n","Ticker Export Complete: SCI\n","End of KNN 1-Train NoteBook Process!\n","Predictions and Export Successful\n"]}],"source":["# iterate_and_export_per_ticker\n","iterate_and_export_per_ticker(output_location, selected_model, model_config_dict, parameters, train_df, test_df, predicted_feature, active_fold, engineered_features)\n","\n","print(f'End of {selected_model} {active_fold} NoteBook Process!')\n","print(\"Predictions and Export Successful\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}